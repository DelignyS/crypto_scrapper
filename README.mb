LA GROSSE EXPLICATION DE MON APPLI Crypto_scrapper

ETAPE 1 : La config de l'environnement
    require 'nokogiri'
    require 'rest-client'

    Nous permettent de récup le contenu html(rest client), et de le traiter(Nokogiri)


ETAPE 2 : Fonction de Scraping

    1.top_10_crypto_scraper ==> On prend l'url "https://coinmarketcap.com/all/views/all/".
    2.Avec Nokogiri::HTML(response) on annalyse le contenu HTML et le converti en 'objet Nokogiri" qui est parcourable.
    3. RestClient.get(url) pour obtenir le contenu HTML de la page
    4.Un tableau vide crypto_data dans lequel on va tout mettre, les données du scrap
    5.On utilise Xpath pour parcourir le tableau des crypto, row après row et cappé à 10, et après la row on pointe spécifiquement sur le name et le price sur la page et on utilise 
    doc.xpath pour obtenir une liste d'élèment scorrespondant à ces Xpaths
    ####### crypto_price = crypto_price_element.text.strip.gsub(/[^\d.]/, '').to_f permet avec une expression regulière de typer crypto_price en float
    
    6.Pour chaques éléments du tableau on prend, nom et prix de la crypto, avec un xpath spécifique à la page.
    7.On concat le nom et le prix dans le hash avec key:name et value:price
    8. On ajout ce hash au tableau crypto_data
    9.la fonction renvoi le tableau crypto_data

ETAPE 3 : Appel de la fonction de Scrapping


    On appel la fonction pour recuperer ses données et on le stock dans la variable
    'top_10_crypto_data = top_10_crypto_scraper'

ETAPE 4 : Affichage du résultat

    On parcours le tableau et on affiche les 10 top cryptos avec 
        top_10_crypto_data.each_with_index do |crypto, index|
     puts "Crypto ##{index + 1}: #{crypto[:name]}, Prix: #{crypto[:price]}"
    end

    ça les affiches à la suite.

    LA GROSSE EXPLICATION DE MON SPEC Crypto_scrapper_spec


ETAPE 1 : Vérifier que le rendu est un tableau

  expect (crypto_data) to be an array 

ETAPE 2 : Vérifier qu'il y a bien 10 éléments

  expect(crypto_data.length).to be >= 10

ETAPE 3 : Verifier que les éléments du tableau c'est bien un hashage avec Key = name et price

  expect(crypto).to have_key(:name)
      expect(crypto).to have_key(:price)

      LA GROSSE EXPLICATION DE Mairie_christmas.rb

ETAPE 1 : LA PREPARATION

Tout d'abord, nous devons nous préparer. Nous importons
 deux bibliothèques : Nokogiri et Rest-Client. Nokogiri est utilisé pour analyser le HTML des pages web, tandis que
 Rest-Client nous permet de télécharger ces pages.

 require 'nokogiri'
require 'rest-client'

ETAPE 2 : LE SCRAP DES LIENS DES MAIRIE

base_url = 'http://annuaire-des-mairies.com'
region_url = 'http://annuaire-des-mairies.com/val-d-oise.html'
district_doc_html = Nokogiri::HTML(URI.open(region_url))
city_links = district_doc_html.css('a.lientxt')

Nous parcourons ensuite les liens de chaque mairie et stockons leur nom et URL dans une liste.

ETAPE 3 : La Collecte des Adresses E-mail :

Maintenant que nous avons la liste des mairies, il est temps de visiter chacune
d'entre elles pour récupérer leur adresse e-mail. Nous itérons sur chaque mairie
et utilisons son URL pour accéder à sa page.

city_links.each do |city_link|
  # ...
  city_doc_html = Nokogiri::HTML(URI.open(city_url))

  En utilisant Nokogiri, nous recherchons l'élément contenant l'adresse e-mail 
  en cherchant le texte "Adresse Email". Une fois que nous avons l'adresse, nous 
  l'ajoutons à notre liste de résultats.

email_elements = city_doc_html.css('td:contains("Adresse Email")')
emails = email_elements.map { |element| element.next_element.text.strip }

ETAPE 4 : La Limitation à 20 Mairies

Pour nous assurer de ne pas dépasser notre objectif de 20 mairies, nous nous 
arrêtons dès que notre liste de résultats atteint cette taille.

break if result.length >= 20

ETAPE 5 : L'Affichage des Résultats :

Finalement, nous affichons les résultats avec un petit numéro d'index pour chaque mairie.

result.each_with_index do |data, index|
  data.each do |name, email|
    puts "#{index + 1}. Ville: #{name}, Email: #{email}"
  end
end

